---
title: "Maximum Likelihood Variance Components for Binary Data and Introduction to Model with Correlated Random Effects"
author: "Gauri Phatak"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(nlmixr)
library(lme4)
library(splitstackshape)
library(earth)
library(cubature)

```

## Introduction

At the time of this paper, computing models for binary data were limited in usage due to the intractability of computation. In this paper a class of probit-normal models is introduced. They describe ML and REML estimations using EM algorithm. EM algorithm proposed is similar to continuous normal linear model.
Computations are feasible for any number of structures of random effects and arbitrary number of fixed effects. Till this point ML estimation was described only for nested random effects model. Incorporation of random effects is useful to account for heterogeneity, over dispersion and intra-cluster correlation. In successive paper by Chan & Kuk they consider correlated random effects.

The focus of this paper is on variance estimation in mixed models and BLUP of observed values of random effects. Usual repeated measures model concentrate on fixed effects parameters and treat variance parameters as nuisance.

Before this paper, beta binomial models were used for correlated binary data. But they do not generalize easily to multiple random effects. There are quasi likelihood methods proposed that focus on fixed effects and treat random effects as nuisance parameters. This paper considers a correlated probit model. It is similar to a logit normal models. But, the logit models were intended only for longitudinal data setting.

The model proposed here is simplified version of threshold model. 

## Model

### What is threshold model?

Suppose for given explanatory variables there is a latent variable U that has continuous distribution.  Now suppose there is another variable Y which is a binary variable. A binary response Y = 1 is recorded only when U > threshold. Without loss of generality the threshold is set to 0.   

### What is probit -normal model?

A probit model is used to determine the likelihood of an item will fall into a particular category. Probit models are used to mode standard normal CDFs.

As an example, 
$$ U \sim N(x'\beta, 1) then, $$

But U here is an unobserved continous variable. It is a latent variable, but the effect of this variable Y is observed which is a binary variable.

The output from the model can be interpreted by using cdf of standard normal.

$$\theta_i = P(Y=1|x_i) = \Phi (x_i' \beta)$$
$$\Phi(t) = (2\pi)^{-1/2}\int^t_{-\infty}exp(-\frac{1}{2}z^2)dz$$
*Model with independent Random effects*

Threshold model,

$$Y = X\beta +Z u +\epsilon,$$
$$ W_i = I_{Y_i >0}, i = 1,2,...,n,$$
$$Where, u \sim N(0,D) \;\;\;\; \epsilon \sim N(0, I),$$

Here $\epsilon$ is standard normal since this lends to describing probability of $P(Y=1|X) = \Phi(X^t\beta)$ where $\Phi$ is cdf of standard normal.

$$ X \equiv diag\{1_{m_i}\} , i = 1,2,...,G$$

$$Z \equiv diag\{1_{n_{ij}}\}, j = 1,2,....m_i\;\; and \;\; \beta = \mu$$
u and $\epsilon$ are independent
This model has restriction that negative correlations cannot be model.
The primary interest is estimation of D parameter i.e. the variances of random effects. 

*Model with correlated Random effects*

Model,

$$Y = X\beta + \sum^{R}_{r=1}Z_r u_r +\epsilon$$

As opposed to the model in McCulloch paper, the distribution of $u_r$ is,

$$u_r \sim N_{q_rk_r}(0,I_{q_r} \otimes \Sigma_r)$$

Here, $u_r$ is made up of $q_r$ iid random vectors of $k_r$ dimensions each.

### What is logit model?

Logit model is used to model the odds of success of an event. Logit model uses logistic/sigmoid function as opposed to std normal cdf used by the probit model. This is used to model Logistic regression. 

If Y is a random variable with a normal distribution and P is a standard logistic function, X = P(Y) has a logit-normal distribution and Y = logit(X) = $\frac{X}{1-X}$.

Advantages of probit-normal model over logit-normal model:

1. With a single random effect and only one observation per level of random effect the proposed model reduces to usual probit model (with a different error term for $\epsilon$). This is not the same for logit models. 

2. The marginal mean of $W_i$ has simple representation,

$$E[W_i] = \Phi(x_i^t\beta(z_i^tDz_i +I)^{-1/2})$$
3. The EM algorithm is very close to continuous normal linear model. This is the most important advantage since it facilitates the use of Gibbs sampling to find the ML and REML estimates. 

Going forward the paper follows standard ANOVA model for variance components estimation,

*For Uncorrelated Random effects model*

$$Y = X \beta +\sum_{i=1}^r Z_i u_i +\epsilon$$
$$u_i \sim  N_{q_r}(0,\theta_i I)\;\;\; and \;\;\; \epsilon \sim N(0,I)$$

*For Correlated Random Effects model*

The distribution of complete data is,

\[
  \begin{bmatrix}
    Y \\ u_1 \\ \vdots \\ u_R
  \end{bmatrix}
  \quad
  \sim  N_{n+Q}\{
  \begin{bmatrix}
  X\beta \\ 0 \\ \vdots \\ 0
  \end{bmatrix} ,
  \begin{bmatrix}
      V    & Z_1(I_{q_1} \otimes \Sigma_1) & \cdots & \cdots & Z_R(I_{q_R} \otimes \Sigma_R) \\
      (I_{q_1} \otimes \Sigma_1)Z_1^t & (I_{q_1} \otimes \Sigma_1) & 0 &\cdots & 0 \\
    \vdots & \vdots & \vdots &   \ddots     &  \vdots   \\
      (I_{q_R} \otimes \Sigma_R)Z_R^t    & 0 & 0 & \cdots & (I_{q_R} \otimes \Sigma_R) \\
  \end{bmatrix} \}
\]

Here,

$$V = I_n + \sum^{R}_{r=1} Z_r(I_{q_r} \otimes \Sigma_r)Z_r^t$$

## EM for MLE and REML

### MLE

Reasons for using EM algorithm:

1. Offers framework for estimation similar to Normal theory case.

2. Auto constraints for iterates to be within parameter space.

3. Natural extension to REML estimation.

4. Converges from a wider range of starting values as compared to quasi-Newton algorithm. 

*For uncorrelated random effects*

*For the Maximization step*, 
Complete data is $(Y, u_i) \;\;\; where\;\;\; i =1,2,3...r$ with r random variables. The maximum likelihood estimate for $\theta_i$ is $\frac{u_i^tu_i}{q_i}$ where $q_i$ is total number of levels within a random effect variable. The MLE for $\beta$ i.e. the fixed effects is $(X^t\hat{V}^{-1}X)^{-1}X^t\hat{V}^{-1}Y$. Here $\hat{V}^{-1}$ is the variance of Y calculated using $\hat{\theta}_i$.

$$var(Y) = I + \sum^c_{i=1}\theta_iZ_iZ_i^t$$

*For the Expectation Step*,

$E[Y|W]$ and $E[u_i^tu_i|W]$ can be calculated using the below,

$$E[u_i^tu_i|W] = E[E[u_i^tu_i|Y] |W]$$
To calculate the inner expectation using multivariate normal distribution since the random variables are normally distributed.


$$E[u_i^tu_i|Y] = \theta^2_i(Y-X\beta)^tV^{-1}Z_iZ_i^tV^{-1}(Y-X\beta) + tr(\theta_i I - \theta^2_i Z_i^t V^{-1}Z_i)$$
Using the equation above to calculate $E[u_i^tu_i|W]$  we get,

$$E[u_i^tu_i|W] = \theta_i^2 trV^{-1}Z_iZ_i^tV^{-1}V_{Y|W} +$$
$$\theta_i^2 (\mu_{Y|W} - X\beta)^tV^{-1}Z_iZ_i^tV^{-1} (\mu_{Y|W} - X\beta)+ $$
$$tr(\theta_i I - \theta_i^2Z_i^tV^{-1}Z_i)$$
Here, $V_{Y|W} = Var(Y|W)$ and $\mu_{Y|W} = E[Y|W]$. These are the only extra computations needed for ML estimation for discrete data. These can be calculated using numerical integration or Gibbs sampling. 

### EM for ML estimation

Steps:

1. Obtaining starting values of $\beta^{(0)}$ and $\theta^{(0)}$ set to $m=0$.

2. Expectation steps:

Calculating the expectation step for m.

$$\hat{t}_i^{(m)} = \theta_i^{(m)2} trV^{(m)-1}Z_iZ_i^tV^{(m)-1}V_{Y|W}^{(m)} +$$
$$\theta_i^{(m)2} (\mu_{Y|W}^{(m)} - X\beta^{(m)})^tV^{(m)-1}Z_iZ_i^tV^{(m)-1} (\mu_{Y|W}^{(m)} - X\beta^{(m)})+ $$
$$tr(\theta_i^{(m)} I - \theta_i^{(m)2}Z_i^tV^{(m)-1}Z_i)$$

3. Maximization steps:

$$\theta_i^{m+1} = \hat{t}^{(m)}_i / q_i$$

and $\beta^{m+1}$,

$$\beta^{m+1} = (X^t V^{-1}X)^{-1}X^tV^{-1}\mu_{Y|W}$$

*Maximization update for For Correlated Random Effects*

Fixed effects estimation,

$$\beta^{m+1} = (X^tX)^{-1}X^t(E[Y|W] - \sum_{r=1}^R Z_r E[u_r|W])$$

Random effects estimation,

$$\Sigma_r^{m+1} = (\sum^{q_r}_{j=1}E[u_{rj}u_{rj}^t|W])/q_r$$

4. if convergence is reached $\hat{\theta}=\theta^{m+1}$ and $\hat{\beta} = \beta^{m+1}$ else,  increment m by 1 and return to step 1.

The implementation of EM algorithm is identical to the continuous case.

### EM for REML estimation

For REML we try and maximize the portion of likelihood that depends only on the variance and not on the fixed effects components. Here they have treated fixed effects as random effects for which variance tends to infinity. Setting prior information for fixed effects be 0, the expected value can be calculated as:

$$E[u_i^tu_i|W] = \theta_i^2 trPZ_iZ_i^tPV_{Y|W} +\theta_i^2 \mu_{Y|W}^tPZ_iZ_i^tP\mu_{Y|W}+tr(\theta_i I - \theta_i^2Z_i^tPZ_i)$$
Where $P = V^{-1} - V^{-1}X(X^tV^{-1}X)^{-1}X^tV^{-1}$.

Using the above, the EM steps are,

1. Obtain starting value of $\theta^{(0)}\;\; with \;\;m=0$.

2. Expectation steps:
$$\hat{t}^{(m)}_i=\theta_i^{(m)2} trP^{(m)}Z_iZ_i^tP^{(m)}V_{Y|W}^{(m)} +$$
$$\theta_i^{(m)2} \mu_{Y|W}^{(m)t}P^{(m)}Z_iZ_i^tP^{(m)}\mu_{Y|W}^{(m)}+$$
  $$tr(\theta_i^{(m)} I - \theta_i^{(m)2}Z_i^tP^{(m)2}Z_i)$$
3. Maximization step,

$$\theta^{(m+1)} = \hat{t}^{m}_i / q_i$$
4. if convergence is reached $\hat{\theta}=\theta^{m+1}$ else increment m by 1 and go back to step 2.


For predicting the value of random effects BLUP methodology is used. Hence here for continous data $E[u_i|Y]=\hat{\theta}^2_iZ^t_i\hat{V}^{-1}(Y-X\hat{\beta})$ and for discrete data $E[u_i|Y]=\hat{\theta}^2_iZ^t_i\hat{V}^{-1}(\hat{\mu}_{Y|W}-X\hat{\beta})$. As we can see $Y$ is replaced by $\hat{\mu}_{Y|W}$

## Examples

### The Weil Data

$$Y_{ijk} = \mu_i + u_{ij} + \epsilon_{ijk} \;\;and\;\; W_{ij}  = I_{Y_{ij} > 0}$$

Here Y is the unobserved latent variables and W is indicator variable. 

The models for the two groups i.e. the treatment and control group are fit separately. Hence the model for treatment is,

$$Y_{tjk} = \mu_t + u_{tj} + \epsilon_{tjk} \;\;and\;\; W_{tj}  = I_{Y_{tj} > 0}$$

and model for control is,
 
$$Y_{cjk} = \mu_c + u_{cj} + \epsilon_{cjk} \;\;and\;\; W_{cj}  = I_{Y_{cj} > 0}$$

Using EM for ML estimation the estimates for treatment group $\hat{\mu}_t = 1.306$ and $\hat{\sigma}_t =0.2403$ and estimates for control group is $\hat{\mu}_t = 0.946$ and $\hat{\sigma}_t =1.023$. This output was compared with output from quasi-newton algorithm for approximation.It was found that for simpler models the EM algorithms converge from wider ranges. 

### Salamander data

The model used here is more complicated as compared to the previous one. Given this, numerical integration to calculate the dependent expectation and variance is not possible. In this paper they used Gibbs sampling to calculate the values and use in EM algorithm. This model uses 2 crossed random effects and 4 fixed effects.

this data consists of 120 data points for 3 experiments. $W_i=1$ if $i^{th}$ mating is successful. There are 20 males and 20 females of two different species R and W. ten of each species of each gender. Female and male form the two random effects. The four crosses i.e. Rf-Rm, Rf-Wm, Wf-Rm, Wf-Wm are the fixed effects. The model can be expressed as,

$$Y=X\beta+Z_fU_f+Z_mU_m+\epsilon$$
$$Here, \;\; W_i = I_{Y_i >0}\;\; and,$$
$$U_{f} \sim N_{20}(0,\theta_fI) \;\;\;, \;\;\; U_{m} \sim N_{20}(0,\theta_mI) \;\;\; and \;\;\; \epsilon \sim N_{120}(0,I)$$

X is the fixed effect matrix, $Z_f$ is the random effects matrix for female and $Z_m$ is the random effects matrix for male and $\beta = (\beta_{R/R},\beta_{R/W},\beta_{W/R},\beta_{W/W})$. 
the final estimates for the parameter values are: $\hat{\beta}_{R/R} = 0.819,\hat{\beta}_{R/W}=0.538,\hat{\beta}_{W/R}=-0.978,\hat{\beta}_{W/W}=0.707, \hat{\theta}_f = 0.6 \;\; and \;\; \hat{\theta}_m = 0.067$

*Salamander data analysis using Correlated Random Effects* 

The data gathered for this experiments had 3 setups. The same set of animals were used for the first two experiments. The assumption for this model is that the the random effects corresponding to the same animals are correlated. 

The model of combined data set for the three experiments includes,

$$Y=X\beta+Z_{f12}U_{f12}+Z_{m12}U_{m12}+Z_{f3}U_{f3}+Z_{m3}U_{m3}+\epsilon$$

The fixed effects are the same as the model from McCullough paper.

the distribution for random effects are,

$$u_{f12} \sim N_{40}(0,I_{20}\otimes \Sigma_{f12}), \;\;\; where\;\;\; \Sigma_{f12} = \begin{matrix}
\sigma^2_{f1} & \sigma_{f12} \\
\sigma_{f12} & \sigma^2_{f2} 
\end{matrix} $$

$$u_{m12} \sim N_{40}(0,I_{20}\otimes \Sigma_{m12}), \;\;\; where\;\;\; \Sigma_{m12} = \begin{matrix}
\sigma^2_{m1} & \sigma_{m12} \\
\sigma_{m12} & \sigma^2_{m2} 
\end{matrix} $$
$$u_{f3} \sim N_{40}(0,\sigma^2_{f3}I_{20}),\;\;\; u_{m3} \sim N_{40}(0,\sigma^2_{m3}I_{20})  $$


## Extension to model to allow for correlation error 

The correlation error can be modeled such that $\epsilon \sim N(0, \Psi), \;\;where \;\; \Psi \neq I$.

For longitudinal data collected for subjects over time the correlation matrix can be,



\[
\Psi_{n_i}(\rho) =
  \begin{bmatrix}
      1 & \rho & \cdots & \rho^{n_i-1}\\
      \rho & 1 & \cdots & \rho^{n_i-1} \\
      \vdots & \vdots & \ddots & \vdots \\
      \rho^{n_i-1} & \rho^{n_i-1} & \cdots & 1 \\
  \end{bmatrix}
\]

The correlation matrix for $\epsilon$ is a block diagonal given by,

$$\Psi(\rho) = diag(\Psi_{n_1}(\rho),\Psi_{n_2}(\rho),\Psi_{n_3}(\rho),....,\Psi_{n_m}(\rho))$$

The estimation for MLE can be considered to be the same as model without non-identity error variances. The fixed effects MLE will change to, 

$$\hat{\beta}(\rho) = (X^t\Psi(\rho)^{-1}X)^{-1}X^t\Psi(\rho)^{-1}(Y - \sum^R_{r=1}Z_ru_r)$$

The MLE of $\hat{\rho}_c$ can be obtained my maximizing profile log-likelihood.

This method can also be extended to multivariate clustered binary data which occurs when studying multiple binary traits for study subjects.


## Try coding for the rat example

```{r, include=FALSE}

data("rats")
dat <- rats
head(dat)
dat$W <- (dat$m - dat$x) > 0
dat$W <- as.numeric(dat$W)
dat$diff <- dat$m - dat$x

## Initialize the values of params
## X matrix : diag matrix of number of groups so 2 groups in weil data case.
dato <- dat
dat <- dat[dat$trt == "t",]
X <- as.matrix(expandRows(as.data.frame(cbind(dat$x2, dat$m)), "V2"))

## Z matrix : diag matrix of number of values within each group.
## values greater than or eq to 1
Trt <- factor(x=rep(x=dat$ID, times=dat$m))
Z <- model.matrix(~Trt-1, data=Trt)

## Output y
y <- expand.bpairs(x+diff ~., dat)
yDat <- as.numeric(y$x)

## Calculate V

V <- function(theta, Z){
  val <- diag(1, nrow=(length(Z[,1]))) + theta * Z %*% t(Z)
  return(val)
}

ll <- function(n, V, mu, X){
  op <- (-1*n/2*log(det(V))- t(X - mu) %*% solve(V) %*% (X-mu))[1,1]
  return(op)
}

f <- function(x, mu1 ,Vinv, yDat) { 
  x * exp((-0.5 *t(x - X * mu1) %*% Vinv %*% (x - X * mu1)) +  
                               log(pnorm(X*mu1))*yDat)
  }
  
f2 <- function(x,mu1, Vinv, yDat ) { 
  x^2 * exp((-0.5 *t(x - X * mu1) %*% Vinv %*% (x - X * mu1)) +  
                               log(pnorm(X*mu1))*yDat)
  }
  
## initial params
mu1 <- 1
mu2 <- 1
s1 <- 0.5
s2 <- 1
pi1 <- sum(dat$W == 0 )/length(dat$W)
pi2 <- sum(dat$W == 1 )/length(dat$W)


## Expectation step
s <- s1
mu <- mu1
Q <- 0
k <- 2


Q[2] <- ll(length(yDat), V(s1,Z), mean(yDat), yDat) 
 
iter  = 0

while (abs(Q[k]-Q[k-1])>=1e-6){
  
  iter <- iter +1
  
  ## Expectation step
  Vinv <- solve(V(s1, Z))
  mid <- Vinv %*% Z %*% t(Z) %*% Vinv
  muy_w <- adaptIntegrate(f,  lower = rep(0, length(yDat)), upper = rep(1, length(yDat)),
                          mu1, Vinv, yDat)
  
  #print("muy_w")
  #print(muy_w)
  mid2 <- muy_w$integral  - X * mu1
  Vy_w <-adaptIntegrate(f2,  lower = rep(0, length(yDat)), upper = rep(1, length(yDat)),
                        mu1, Vinv, yDat)
  t1 <- s1^2 * sum(diag( mid * Vy_w$integral ))+
  s1^2 * t(mid2) %*% mid %*% mid2+
  sum(diag(s1* diag(1, nrow = dim(Z)[2]) - s1^2 * t(Z) %*% Vinv %*% Z ))
    
  ## Maximization step
  s1 <- (t1 / length(dat$ID))[1,1] 
  s[k] <- s1
  Vinv <- solve(V(s1, Z))
  mu1 <- sum(solve(t(X) %*% Vinv %*% X) %*% t(X) %*% Vinv * muy_w$integral) / length(mu1)
  mu[k] <- mu1
  #print("mu1")
  #print(mu1)
  k <- k+1
  Q[k] <-  ll(length(yDat), V(s1,Z), mu1 , yDat) 
  
  if(iter > 500){
    break
  }
    
}


```

The Output from SAS which uses Newton-Rhapson approximation method is as given below,

![Estimation Output for Weil Data](WeilDat.png)

As we can see it is very similar to the output from the EM method.


## Conclusion and succesive extension

This paper gives a framework for ML and REML estimation for Binary data using EM algorithm. It is similar to EM for continuous data. The limiting factor in this paper is using random effects as uncorrelated. Extension of the work done in the paper can be seen in paper by Chan and Kuk. In their paper they introduce an estimation method for mixed models with correlated random effects. 


## References
https://rpubs.com/H_Zhu/246450

https://ecommons.cornell.edu/bitstream/handle/1813/31608/BU-1037-MD.pdf;jsessionid=F56250DB8EF10F2BB7E4F579EB98D211?sequence=1


https://www.maths.usyd.edu.au/u/jchan/GLM/1997Biometric.pdf

https://support.sas.com/documentation/cdl/en/statug/63033/HTML/default/viewer.htm#statug_nlmixed_sect040.htm



